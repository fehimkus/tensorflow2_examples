<html>
<head>
<title>src.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #0033b3;}
.s1 { color: #080808;}
.s2 { color: #1750eb;}
.s3 { color: #008080; font-weight: bold;}
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
src.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">os</span>
<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">tensorflow </span><span class="s0">as </span><span class="s1">tf</span>


<span class="s1">tf.compat.v1.disable_eager_execution()</span>

<span class="s0">class </span><span class="s1">OUActionNoise(object):</span>
  <span class="s0">def </span><span class="s1">__init__(self, mu, sigma=</span><span class="s2">0.15</span><span class="s1">, theta=</span><span class="s2">0.2</span><span class="s1">, dt=</span><span class="s2">1e-2</span><span class="s1">, x0=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s1">self.theta = theta</span>
    <span class="s1">self.mu = mu</span>
    <span class="s1">self.dt = dt</span>
    <span class="s1">self.sigma = sigma</span>
    <span class="s1">self.x0 = x0</span>
    <span class="s1">self.reset()</span>

  <span class="s0">def </span><span class="s1">__call__(self):</span>
    <span class="s1">x = self.x_prev + self.theta*(self.mu-self.x_prev)*self.dt + \</span>
    <span class="s1">self.sigma*np.sqrt(self.dt)*np.random.normal(size=self.mu.shape)</span>
    <span class="s1">self.x_prev = x</span>
    <span class="s0">return </span><span class="s1">x</span>

  <span class="s0">def </span><span class="s1">reset(self):</span>
    <span class="s1">self.x_prev = self.x0 </span><span class="s0">if </span><span class="s1">self.x0 </span><span class="s0">is not None else </span><span class="s1">np.zeros_like(self.mu)</span>

<span class="s0">class </span><span class="s1">ReplayBuffer(object):</span>
  <span class="s0">def </span><span class="s1">__init__(self, max_size, input_shape, n_actions):</span>
    <span class="s1">self.mem_size = max_size</span>
    <span class="s1">self.mem_cntr = </span><span class="s2">0</span>
    <span class="s1">self.state_memory = np.zeros((self.mem_size, *input_shape))</span>
    <span class="s1">self.new_state_memory = np.zeros((self.mem_size, *input_shape))</span>
    <span class="s1">self.action_memory = np.zeros((self.mem_size, n_actions))</span>
    <span class="s1">self.reward_memory = np.zeros(self.mem_size)</span>
    <span class="s1">self.terminal_memory = np.zeros(self.mem_size, dtype = np.float32)</span>

  <span class="s0">def </span><span class="s1">store_transition(self, state, action, reward, state_, done):</span>
    <span class="s1">index = self.mem_cntr % self.mem_size</span>
    <span class="s1">self.state_memory[index] = state</span>
    <span class="s1">self.new_state_memory[index] =state_</span>
    <span class="s1">self.reward_memory[index] = reward</span>
    <span class="s1">self.action_memory[index] = action</span>
    <span class="s1">self.terminal_memory[index] = </span><span class="s2">1</span><span class="s1">- int(done)</span>
    <span class="s1">self.mem_cntr =+ </span><span class="s2">1</span>

  <span class="s0">def </span><span class="s1">sample_buffer(self, batch_size):</span>
    <span class="s1">max_mem = min(self.mem_cntr, self.mem_size)</span>
    <span class="s1">batch = np.random.choice(max_mem, batch_size)</span>

    <span class="s1">states = self.state_memory[batch]</span>
    <span class="s1">new_states = self.new_state_memory[batch]</span>
    <span class="s1">rewards = self.reward_memory[batch]</span>
    <span class="s1">actions = self.action_memory[batch]</span>
    <span class="s1">terminal = self.terminal_memory[batch]</span>

    <span class="s0">return </span><span class="s1">states, actions, rewards, new_states, terminal</span>

<span class="s0">class </span><span class="s1">Actor(object):</span>
  <span class="s0">def </span><span class="s1">__init__(self, lr, n_actions, name, input_dims, sess, fc1_dims,</span>
               <span class="s1">fc2_dims, action_bound, batch_size=</span><span class="s2">64</span><span class="s1">, chkpt_dir = </span><span class="s3">'temp/ddpg'</span><span class="s1">):</span>
    <span class="s1">self.lr = lr</span>
    <span class="s1">self.n_actions = n_actions</span>
    <span class="s1">self.name = name</span>
    <span class="s1">self.input_dims =input_dims</span>
    <span class="s1">self.fc1_dims = fc1_dims</span>
    <span class="s1">self.fc2_dims = fc2_dims</span>
    <span class="s1">self.sess = sess</span>
    <span class="s1">self.batch_size = batch_size</span>
    <span class="s1">self.action_bound = action_bound</span>
    <span class="s1">self.chkpt_dir = chkpt_dir</span>
    <span class="s1">self.build_network()</span>
    <span class="s1">self.params = tf.compat.v1.trainable_variables(scope=self.name)</span>
    <span class="s1">self.saver = tf.compat.v1.train.Saver()</span>
    <span class="s1">self.checkpoint_file = os.path.join(chkpt_dir, name+ </span><span class="s3">'_ddpg.ckpt'</span><span class="s1">)</span>

    <span class="s1">self.unnormalized_actor_gradients = tf.compat.v1.gradients(self.mu, self.params,</span>
                                                     <span class="s1">-self.action_gradient)</span>

    <span class="s1">self.actor_gradients = list(map(</span><span class="s0">lambda </span><span class="s1">x: tf.compat.v1.div(x, self.batch_size),</span>
                                    <span class="s1">self.unnormalized_actor_gradients))</span>

    <span class="s1">self.optimize =tf.compat.v1.train.AdamOptimizer(self.lr).apply_gradients(zip(self.actor_gradients, self.params))</span>

  <span class="s0">def </span><span class="s1">build_network(self):</span>
    <span class="s0">with </span><span class="s1">tf.compat.v1.variable_scope(self.name):</span>
      <span class="s1">self.input = tf.compat.v1.placeholder(tf.float32,</span>
                                 <span class="s1">shape = [</span><span class="s0">None</span><span class="s1">, *self.input_dims],</span>
                                 <span class="s1">name = </span><span class="s3">'inputs'</span><span class="s1">)</span>
      <span class="s1">self.action_gradient = tf.compat.v1.placeholder(tf.float32,</span>
                                            <span class="s1">shape = [</span><span class="s0">None</span><span class="s1">, self.n_actions])</span>
      <span class="s1">f1 = </span><span class="s2">1</span><span class="s1">/np.sqrt(self.fc1_dims)</span>
      <span class="s1">dense1 = tf.compat.v1.layers.dense(self.input, units=self.fc1_dims,</span>
                               <span class="s1">kernel_initializer = tf.compat.v1.initializers.random_uniform(-f1, f1),</span>
                               <span class="s1">bias_initializer = tf.compat.v1.initializers.random_uniform(-f1, f1))</span>
      <span class="s1">batch1 = tf.compat.v1.layers.batch_normalization(dense1)</span>
      <span class="s1">layer1_activation = tf.nn.relu(batch1)</span>

      <span class="s1">f2 = </span><span class="s2">1</span><span class="s1">/ np.sqrt(self.fc2_dims)</span>
      <span class="s1">dense2 = tf.compat.v1.layers.dense(layer1_activation, units=self.fc2_dims,</span>
                               <span class="s1">kernel_initializer = tf.compat.v1.initializers.random_uniform(-f2, f2),</span>
                               <span class="s1">bias_initializer = tf.compat.v1.initializers.random_uniform(-f2, f2))</span>
      <span class="s1">batch2 = tf.compat.v1.layers.batch_normalization(dense2)</span>
      <span class="s1">layer2_activation = tf.nn.relu(batch2)</span>

      <span class="s1">f3 = </span><span class="s2">0.003</span>
      <span class="s1">mu = tf.compat.v1.layers.dense(layer2_activation, units=self.n_actions,</span>
                           <span class="s1">activation = </span><span class="s3">'tanh'</span><span class="s1">,</span>
                           <span class="s1">kernel_initializer = tf.compat.v1.initializers.random_uniform(-f3, f3),</span>
                           <span class="s1">bias_initializer = tf.compat.v1.initializers.random_uniform(-f3, f3))</span>

      <span class="s1">self.mu= tf.multiply(mu, self.action_bound)</span>

  <span class="s0">def </span><span class="s1">predict(self, inputs):</span>
    <span class="s0">return </span><span class="s1">self.sess.run(self.mu, feed_dict= {self.input: inputs})</span>

  <span class="s0">def </span><span class="s1">train(self, inputs, gradients):</span>
    <span class="s1">self.sess.run(self.optimize,</span>
                  <span class="s1">feed_dict ={self.input: inputs,</span>
                              <span class="s1">self.action_gradient: gradients})</span>

  <span class="s0">def </span><span class="s1">save_checkpoint(self):</span>
    <span class="s1">print(</span><span class="s3">'... saving checkpoint ...'</span><span class="s1">)</span>
    <span class="s1">self.saver.save(self.sess, self.checkpoint_file)</span>

  <span class="s0">def </span><span class="s1">load_checkpoint(self):</span>
    <span class="s1">print(</span><span class="s3">'... loading checkpoint ...'</span><span class="s1">)</span>
    <span class="s1">self.saver.restore(self.sess, self.checkpoint_file)</span>


<span class="s0">class </span><span class="s1">Critic(object):</span>
    <span class="s0">def </span><span class="s1">__init__(self, lr, n_actions, name, input_dims, sess, fc1_dims,</span>
               <span class="s1">fc2_dims, batch_size=</span><span class="s2">64</span><span class="s1">, chkpt_dir = </span><span class="s3">'temp/ddpg'</span><span class="s1">):</span>
      <span class="s1">self.lr = lr</span>
      <span class="s1">self.n_actions = n_actions</span>
      <span class="s1">self.name = name</span>
      <span class="s1">self.input_dims =input_dims</span>
      <span class="s1">self.fc1_dims = fc1_dims</span>
      <span class="s1">self.fc2_dims = fc2_dims</span>
      <span class="s1">self.sess = sess</span>
      <span class="s1">self.batch_size = batch_size</span>
      <span class="s1">self.chkpt_dir = chkpt_dir</span>
      <span class="s1">self.build_network()</span>
      <span class="s1">self.params = tf.compat.v1.trainable_variables(scope=self.name)</span>
      <span class="s1">self.saver = tf.compat.v1.train.Saver()</span>
      <span class="s1">self.checkpoint_file = os.path.join(chkpt_dir, name+ </span><span class="s3">'_ddpg.ckpt'</span><span class="s1">)</span>

      <span class="s1">self.optimize = tf.compat.v1.train.AdamOptimizer(self.lr).minimize(self.loss)</span>

      <span class="s1">self.action_gradients = tf.gradients(self.q, self.actions)</span>

    <span class="s0">def </span><span class="s1">build_network(self):</span>
      <span class="s0">with </span><span class="s1">tf.compat.v1.variable_scope(self.name):</span>
        <span class="s1">self.input = tf.compat.v1.placeholder(tf.float32,</span>
                                 <span class="s1">shape = [</span><span class="s0">None</span><span class="s1">, *self.input_dims],</span>
                                 <span class="s1">name = </span><span class="s3">'inputs'</span><span class="s1">)</span>
        <span class="s1">self.actions = tf.compat.v1.placeholder(tf.float32,</span>
                                 <span class="s1">shape = [</span><span class="s0">None</span><span class="s1">, self.n_actions],</span>
                                 <span class="s1">name = </span><span class="s3">'actions'</span><span class="s1">)</span>
        <span class="s1">self.q_target = tf.compat.v1.placeholder(tf.float32,</span>
                                 <span class="s1">shape = [</span><span class="s0">None</span><span class="s1">, </span><span class="s2">1</span><span class="s1">],</span>
                                 <span class="s1">name = </span><span class="s3">'target'</span><span class="s1">)</span>

      <span class="s1">f1 = </span><span class="s2">1</span><span class="s1">/np.sqrt(self.fc1_dims)</span>
      <span class="s1">dense1 = tf.compat.v1.layers.dense(self.input, units=self.fc1_dims,</span>
                               <span class="s1">kernel_initializer = tf.compat.v1.initializers.random_uniform(-f1, f1),</span>
                               <span class="s1">bias_initializer = tf.compat.v1.initializers.random_uniform(-f1, f1))</span>
      <span class="s1">batch1 = tf.compat.v1.layers.batch_normalization(dense1)</span>
      <span class="s1">layer1_activation = tf.nn.relu(batch1)</span>

      <span class="s1">f2 = </span><span class="s2">1</span><span class="s1">/ np.sqrt(self.fc2_dims)</span>
      <span class="s1">dense2 = tf.compat.v1.layers.dense(layer1_activation, units=self.fc2_dims,</span>
                               <span class="s1">kernel_initializer = tf.compat.v1.initializers.random_uniform(-f2, f2),</span>
                               <span class="s1">bias_initializer = tf.compat.v1.initializers.random_uniform(-f2, f2))</span>
      <span class="s1">batch2 = tf.compat.v1.layers.batch_normalization(dense2)</span>

      <span class="s1">action_in = tf.compat.v1.layers.dense(self.actions, units=self.fc2_dims,</span>
                                  <span class="s1">activation = </span><span class="s3">'relu'</span><span class="s1">)</span>

      <span class="s1">state_actions = tf.add(batch2, action_in)</span>
      <span class="s1">state_actions = tf.nn.relu(state_actions)</span>

      <span class="s1">f3 = </span><span class="s2">0.003</span>
      <span class="s1">self.q = tf.compat.v1.layers.dense(state_actions, units =</span><span class="s2">1</span><span class="s1">,</span>
                               <span class="s1">kernel_initializer = tf.compat.v1.initializers.random_uniform(-f3, f3),</span>
                               <span class="s1">bias_initializer = tf.compat.v1.initializers.random_uniform(-f3, f3),</span>
                               <span class="s1">kernel_regularizer= tf.keras.regularizers.l2(</span><span class="s2">0.01</span><span class="s1">))</span>

      <span class="s1">self.loss = tf.losses.mean_squared_error(self.q_target, self.q)</span>

    <span class="s0">def </span><span class="s1">predict(self, inputs, actions):</span>
        <span class="s0">return </span><span class="s1">self.sess.run(self.q, feed_dict={self.input: inputs, self.actions: actions})</span>

    <span class="s0">def </span><span class="s1">train(self, inputs, actions, q_target):</span>
        <span class="s0">return </span><span class="s1">self.sess.run(self.optimize,</span>
                  <span class="s1">feed_dict ={self.inputs: inputs,</span>
                              <span class="s1">self.actions: actions,</span>
                              <span class="s1">self.q_target: q_target})</span>

    <span class="s0">def </span><span class="s1">get_action_gradients(self, inputs, actions):</span>
        <span class="s0">return </span><span class="s1">self.sess.run(self.action_gradients,</span>
                  <span class="s1">feed_dict ={self.inputs: inputs,</span>
                              <span class="s1">self.actions: actions})</span>

    <span class="s0">def </span><span class="s1">save_checkpoint(self):</span>
        <span class="s1">print(</span><span class="s3">'... saving checkpoint ...'</span><span class="s1">)</span>
        <span class="s1">self.saver.save(self.sess, self.checkpoint_file)</span>

    <span class="s0">def </span><span class="s1">load_checkpoint(self):</span>
        <span class="s1">print(</span><span class="s3">'... loading checkpoint ...'</span><span class="s1">)</span>
        <span class="s1">self.saver.restore(self.sess, self.checkpoint_file)</span>

<span class="s0">class </span><span class="s1">Agent(object):</span>
  <span class="s0">def </span><span class="s1">__init__(self, alpha, beta, input_dims, tau, env, gamma=</span><span class="s2">0.99</span><span class="s1">, n_actions=</span><span class="s2">2</span><span class="s1">, max_size=</span><span class="s2">1000000</span><span class="s1">, layer1_size=</span><span class="s2">400</span><span class="s1">, layer2_size=</span><span class="s2">300</span><span class="s1">, batch_size=</span><span class="s2">64</span><span class="s1">):</span>
    <span class="s1">self.gamma = gamma</span>
    <span class="s1">self.tau = tau</span>
    <span class="s1">self.memory = ReplayBuffer(max_size, input_dims, n_actions)</span>
    <span class="s1">self.batch_size = batch_size</span>
    <span class="s1">self.sess = tf.compat.v1.Session()</span>

    <span class="s1">self.actor = Actor(alpha, n_actions, </span><span class="s3">'Actor'</span><span class="s1">, input_dims, self.sess,</span>
                       <span class="s1">layer1_size, layer2_size, env.action_space.high)</span>

    <span class="s1">self.critic = Critic(beta, n_actions, </span><span class="s3">'Critic'</span><span class="s1">, input_dims, self.sess,</span>
                       <span class="s1">layer1_size, layer2_size)</span>

    <span class="s1">self.target_actor = Actor(alpha, n_actions, </span><span class="s3">'TargetActor'</span><span class="s1">, input_dims,</span>
                              <span class="s1">self.sess, layer1_size, layer2_size,</span>
                              <span class="s1">env.action_space.high)</span>

    <span class="s1">self.target_critic = Critic(beta, n_actions, </span><span class="s3">'TargetCritic'</span><span class="s1">, input_dims,</span>
                                <span class="s1">self.sess, layer1_size, layer2_size)</span>

    <span class="s1">self.noise = OUActionNoise(mu = np.zeros(n_actions))</span>

    <span class="s1">self.update_critic = \</span>
        <span class="s1">[self.target_critic.params[i].assign(</span>
            <span class="s1">tf.multiply(self.critic.params[i], self.tau) \</span>
            <span class="s1">+ tf.multiply(self.target_critic.params[i], </span><span class="s2">1. </span><span class="s1">-self.tau))</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(len(self.target_critic.params))]</span>

    <span class="s1">self.update_actor = \</span>
        <span class="s1">[self.target_actor.params[i].assign(</span>
            <span class="s1">tf.multiply(self.actor.params[i], self.tau) \</span>
            <span class="s1">+ tf.multiply(self.target_actor.params[i], </span><span class="s2">1. </span><span class="s1">-self.tau))</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(len(self.target_actor.params))]</span>

    <span class="s1">self.sess.run(tf.compat.v1.global_variables_initializer())</span>

    <span class="s1">self.update_network_parameters(first = </span><span class="s0">True</span><span class="s1">)</span>

  <span class="s0">def </span><span class="s1">update_network_parameters(self, first=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s0">if </span><span class="s1">first:</span>
      <span class="s1">old_tau = self.tau</span>
      <span class="s1">self.tau = </span><span class="s2">1.0</span>
      <span class="s1">self.target_critic.sess.run(self.update_critic)</span>
      <span class="s1">self.target_actor.sess.run(self.update_actor)</span>
      <span class="s1">self.tau = old_tau</span>
    <span class="s0">else</span><span class="s1">:</span>
      <span class="s1">self.target_critic.sess.run(self.update_critic)</span>
      <span class="s1">self.target_actor.sess.run(self.update_actor)</span>

  <span class="s0">def </span><span class="s1">Remember(self, state, action, reward, new_state, done):</span>
    <span class="s1">self.memory.store_transition(state, action, reward, new_state, done)</span>

  <span class="s0">def </span><span class="s1">choose_action(self, state):</span>
    <span class="s1">state =state[np.newaxis, :]</span>
    <span class="s1">mu = self.actor.predict(state)</span>
    <span class="s1">noise = self.noise()</span>
    <span class="s1">mu_prime = mu + noise</span>

    <span class="s0">return </span><span class="s1">mu_prime[</span><span class="s2">0</span><span class="s1">]</span>

  <span class="s0">def </span><span class="s1">learn(self):</span>
    <span class="s0">if </span><span class="s1">self.memory.mem_cntr &lt; self.batch_size:</span>
      <span class="s0">return</span>
    <span class="s1">state, action, reward, new_state, done = \</span>
    <span class="s1">self.memory.sample_buffer(self.batch_size)</span>

    <span class="s1">critic_value_ =self.target_critic.predict(new_state,</span>
                                              <span class="s1">self.target_actor.predict(new_state))</span>

    <span class="s1">target = []</span>
    <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(self.batch_size):</span>
      <span class="s1">target.append(reward[j] + self.gamma*critic_value_[j]*done[j])</span>

    <span class="s1">target = np.reshape(target, (self.batch_size, </span><span class="s2">1</span><span class="s1">))</span>

    <span class="s1">_ = self.critic.train(state, action, target)</span>

    <span class="s1">a_outs = self.actor.predict(state)</span>
    <span class="s1">grads = self.critic.get_action_gradients(state, a_outs)</span>
    <span class="s1">self.actor.train(state, grads[</span><span class="s2">0</span><span class="s1">])</span>

    <span class="s1">self.update_networkparameters()</span>

  <span class="s0">def </span><span class="s1">save_models(self):</span>
    <span class="s1">self.actor.save_checkpoint()</span>
    <span class="s1">self.target_actor.save_checkpoint()</span>
    <span class="s1">self.critic.save_checkpoint()</span>
    <span class="s1">self.target_critic.save_checkpoint()</span>

  <span class="s0">def </span><span class="s1">load_models(self):</span>
    <span class="s1">self.actor.load_checkpoint()</span>
    <span class="s1">self.target_actor.load_checkpoint()</span>
    <span class="s1">self.critic.load_checkpoint()</span>
    <span class="s1">self.target_critic.load_checkpoint()</span>
</pre>
</body>
</html>